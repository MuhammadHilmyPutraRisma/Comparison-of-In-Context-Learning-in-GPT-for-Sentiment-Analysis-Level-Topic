{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Library Preparation"
      ],
      "metadata": {
        "id": "XV19g0Zkl-D7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86IxgjP5koiy"
      },
      "outputs": [],
      "source": [
        "# Clone repository dan instalasi dependensi RapidsAI\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "\n",
        "# Instalasi library tambahan\n",
        "!pip install app_store_scraper\n",
        "!pip install openai\n",
        "\n",
        "# Import library untuk clustering dan visualisasi\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from cuml.cluster import KMeans\n",
        "\n",
        "# Import library untuk analisis data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Import library untuk pemrosesan NLP\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Import library untuk scraping ulasan\n",
        "from app_store_scraper import AppStore\n",
        "\n",
        "# Mount Google Drive untuk penyimpanan file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Menambahkan path ke dalam sistem\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/Deep Learning/FCMeans\")\n",
        "\n",
        "# Import library tambahan untuk clustering\n",
        "from fcmeans import fcmeans\n",
        "\n",
        "# Import library tambahan untuk NLP\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "\n",
        "# Instalasi OpenAI\n",
        "import openai\n",
        "\n",
        "# Fungsi tambahan\n",
        "import re\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install app_store_scraper"
      ],
      "metadata": {
        "id": "AaObcHQT6gdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengecek versi library yang terpasang\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import gensim\n",
        "import sklearn\n",
        "import openai\n",
        "import tqdm\n",
        "import re\n",
        "import sys\n",
        "import app_store_scraper\n",
        "from transformers import __version__ as transformers_version\n",
        "# Output versi\n",
        "versions = {\n",
        "    \"pandas\": pd.__version__,\n",
        "    \"numpy\": np.__version__,\n",
        "    \"tensorflow\": tf.__version__,\n",
        "    \"matplotlib\": matplotlib.__version__,\n",
        "    \"seaborn\": sns.__version__,\n",
        "    \"nltk\": nltk.__version__,\n",
        "    \"gensim\": gensim.__version__,\n",
        "    \"sklearn\": sklearn.__version__,\n",
        "    \"openai\": openai.__version__,\n",
        "    \"tqdm\": tqdm.__version__,\n",
        "    \"re\": re.__version__,\n",
        "    \"transformers\": transformers_version,\n",
        "    \"app_store_scraper\": app_store_scraper.__version__,\n",
        "}\n",
        "\n",
        "# Menampilkan hasil\n",
        "versions_df = pd.DataFrame(list(versions.items()), columns=[\"Library\", \"Version\"])\n",
        "print(versions_df)\n"
      ],
      "metadata": {
        "id": "caVfkOz96HfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Scraping App Store"
      ],
      "metadata": {
        "id": "K-mnYMl-mE-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi objek AppStore untuk aplikasi Tokocrypto\n",
        "data = AppStore(country='id', app_name='Tokocrypto: Trade BTC & Crypto', app_id='1538556690')\n",
        "\n",
        "#Mengatur jumlah ulasan yang akan diambil\n",
        "data.review(how_many=99999)"
      ],
      "metadata": {
        "id": "0qBb8n6Llo8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3RDwVma-ows"
      },
      "outputs": [],
      "source": [
        "data.review # Mengakses daftar ulasan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP7RDfv3-xkJ"
      },
      "outputs": [],
      "source": [
        "data1 = pd.DataFrame(np.array(data.reviews),columns=['review']) # Membuat DataFrame dari ulasan\n",
        "data2 = data1.join(pd.DataFrame(data1.pop('review').tolist())) # Membuat data ulasan menjadi kolom terstruktur"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = data2.sort_values('date', ascending=False) # Mengurutkan ulasan berdasarkan tanggal (terbaru di atas)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "nXFrstZLlCB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq3Fbw4t-yT3"
      },
      "outputs": [],
      "source": [
        "Tokocrypto = data2[['date','userName','review','rating']] # Memilih kolom yang akan digunakan\n",
        "Tokocrypto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RpWXoUEnBWt"
      },
      "outputs": [],
      "source": [
        "Tokocrypto.to_excel('Tokocrypto.xlsx', index=False) # Menyimpan DataFrame sebagai file Excel tanpa indeks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "9Pmu-9YUn-DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"data_tokocrypto.xlsx\")\n",
        "df"
      ],
      "metadata": {
        "id": "kkfv1J9motbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import kamus untuk mengubah singkatan\n",
        "kamus_alay = pd.read_csv('https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv')\n",
        "## Membuat dictionary untuk memetakan singkatan dari kata\n",
        "nor_dict = {}\n",
        "for index, row in kamus_alay.iterrows():\n",
        "  if row[0] not in nor_dict:\n",
        "    nor_dict[row[0]] = row[1]\n",
        "def clean_text_1(text):\n",
        "  # 01 Konversi ke huruf kecil semua\n",
        "  text = text.strip().lower()\n",
        "  # 02 Menghilangkan tautan www.* atau https?://*\n",
        "  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','', text)\n",
        "  # 03 Menghilangkan @username\n",
        "  text = re.sub('@[^\\s]+','', text)\n",
        "  # 04 Menghilangkan # dari #kata\n",
        "  text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "  # 05 Menghilangkan tanda !, ?, +, &\n",
        "  text = re.sub(r'[!&?+%]', '', text)\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  # 07 Menghilangkan spasi berlebihan\n",
        "  text = re.sub('[\\s]+', ' ', text)\n",
        "  text = re.sub(r'[ðÿ]', '', text)\n",
        "  text = re.sub(r'[ðŸ‘]', '', text)\n",
        "  text = re.sub(r'[ðŸ]', '', text)\n",
        "  # 08 Menghilangkan emoticon dan semacamnya\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                              u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
        "                              u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
        "                              u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
        "                              u\"\\U0001F1E0-\\U0001F1FF\" # flags (ios)\n",
        "                              \"]+\", flags=re.UNICODE)\n",
        "  text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "  # Pembersihan kata\n",
        "  words = text.split()\n",
        "  tokens=[]\n",
        "  for ww in words:\n",
        "      # Memisahkan kata berulang\n",
        "      for w in re.split(r'[-/\\s]\\s*', ww):\n",
        "          # Menghapus huruf berulang yang lebih dari dua kali\n",
        "          pattern = re.compile(r\"(.)\\1{1,}\", re. DOTALL)\n",
        "          w = pattern.sub(r\"\\1\\1\", w)\n",
        "          w = w.strip('\\'\"?,.')\n",
        "          # Memeriksa apakah suatu kata terbentuk dari minimal dua huruf\n",
        "          #val = re.search(r\"^[a-zA-Z][a-zA-Z][a-zA-Z]*$\", w)\n",
        "          if w in nor_dict:\n",
        "              w = nor_dict[w]\n",
        "          if w == \"rt\":\n",
        "          #or val is None:\n",
        "              continue\n",
        "          else:\n",
        "            tokens.append(w.lower())\n",
        "  text = \" \".join(tokens)\n",
        "  return text.strip()\n",
        "\n",
        "def clean_text_2(text):\n",
        "    # Untuk CTFIDF\n",
        "    # 01 Menghilangkan tanda baca\n",
        "    text = re.sub(r' [^\\w\\s]','', text)\n",
        "    # 02 Menghilangkan angka\n",
        "    text = re.sub(\"[0-9]\",\"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "## Untuk CTFIDF\n",
        "additional_stopwords = ['gue', 'pas', 'banget', 'bikin', 'gua', 'ya', 'kalo', 'saya', 'nya', 'saat',\n",
        "                        'sih', 'deh', 'dll', 'nih', 'donk', 'min', 'ny', 'si', 'eh', 'tu', 'mah',\n",
        "                        'loh', 'aja', 'bagus','membantu', 'good', 'mantap', 'mudah', 'cepat', 'ðŸ‘', 'alhamdulillah', 'keren',\n",
        "                        'terimakasih','kali','top','best','bad','useless','trash','job','worst']\n",
        "stop_words = stopwords.words('indonesian') + additional_stopwords + stopwords.words('english')\n",
        "stop_words = set(stop_words)\n",
        "\n",
        "def remove_stop_words(s):\n",
        "    return \" \".join(word for word in s.split() if word not in stop_words)"
      ],
      "metadata": {
        "id": "_TnAaEsCo5u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat kolom text_1 sebagai representasi string dari kolom ulasan\n",
        "df[\"text_1\"] = df[\"ulasan\"].astype(str)\n",
        "\n",
        "# Membersihkan teks di kolom text_1 dengan fungsi clean_text_1 dan clean_text_2\n",
        "df[\"text_1\"] = df[\"text_1\"].map(lambda x: clean_text_1(x))\n",
        "df[\"text_2\"] = df[\"text_1\"].map(lambda x: clean_text_2(x))\n",
        "\n",
        "# Menghapus stopwords dari kolom text_2\n",
        "df['text_2'] = df['text_2'].apply(lambda x: remove_stop_words(x))\n",
        "\n",
        "# Memfilter baris berdasarkan panjang teks (lebih dari 3 kata)\n",
        "df = df[df[\"text_1\"].apply(lambda x: len(x.split()) > 3)]\n",
        "\n",
        "# Memfilter baris yang tidak mengandung kata-kata tertentu\n",
        "df = df[~df['text_1'].str.contains('game|hero|match|epep|player|ping|resolusi', case=False)]\n",
        "\n",
        "# Menampilkan DataFrame\n",
        "df"
      ],
      "metadata": {
        "id": "QspBDtDtNlFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-Shot GPT"
      ],
      "metadata": {
        "id": "86Hhht7FBP0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Memisahkan data untuk data testing\n",
        "df_testing = df[df['label'].notna()]\n",
        "df_testing"
      ],
      "metadata": {
        "id": "ywaHGcUhFs5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_from_message(message,\n",
        "                                model=\"gpt-4o\",\n",
        "                                temperature=0):\n",
        "    client = OpenAI(\n",
        "        api_key='OPENAI_API_KEY',\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=message,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def sentiment(ulasan):\n",
        "    delimiter = \"#####\"\n",
        "    system_message = f\"\"\"Saya sedang melakukan analisis sentimen terhadap ulasan pelanggan dari sebuah aplikasi \\\n",
        "    exchange crypto bernama tokocrypto\n",
        "    Petunjuk:\n",
        "    Jawab hanya dengan satu kata saja: \"positif\" atau \"negatif\"!\"\"\"\n",
        "\n",
        "    system_message = {'role': 'system', 'content': system_message}\n",
        "\n",
        "    # Menyiapkan pesan pengguna untuk ulasan yang ingin diuji\n",
        "    user_message = [{'role': 'user', 'content': f\"{delimiter}{ulasan}{delimiter}\"}]\n",
        "\n",
        "    # Gabungkan pesan sistem dan pesan pengguna\n",
        "    messages = [system_message] + user_message\n",
        "\n",
        "    # Dapatkan respons model\n",
        "    response = get_completion_from_message(messages).strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Tambahkan kolom 'sentimen' berdasarkan hasil dari fungsi sentiment\n",
        "df_testing['zero-shot'] = df_testing['text_1'].apply(sentiment)\n",
        "\n",
        "# Cetak dataframe dengan kolom sentimen yang baru\n",
        "print(df_testing)\n"
      ],
      "metadata": {
        "id": "KDp641R4BS7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-Shot Positive"
      ],
      "metadata": {
        "id": "NAp3NLnGCAyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_completion_from_message(message,\n",
        "                                model=\"gpt-4o\",\n",
        "                                temperature=0):\n",
        "    client = OpenAI(\n",
        "        api_key='OPENAI_API_KEY',\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=message,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def sentiment_single(ulasan):\n",
        "    delimiter = \"#####\"\n",
        "\n",
        "    # Menyediakan contoh one-shot positif\n",
        "    example_input = \"UI aplikasinya simple, jadi bikin gabingung buat orang yang baru belajar\"\n",
        "    example_output = \"positif\"\n",
        "\n",
        "    system_message = f\"\"\"Saya sedang melakukan analisis sentimen terhadap ulasan pelanggan dari sebuah aplikasi \\\n",
        "    exchange crypto bernama tokocrypto.\n",
        "    Berikut adalah contoh ulasan dan jawabannya:\n",
        "    Ulasan: {example_input}\n",
        "    Jawaban: {example_output}\n",
        "\n",
        "    Petunjuk:\n",
        "    - Jawab hanya dengan satu kata: \"positif\" atau \"negatif\" berdasarkan ulasan yang diberikan.\n",
        "    - Ulasan: {delimiter}{ulasan}{delimiter}\"\"\"\n",
        "\n",
        "    system_message = {'role': 'system', 'content': system_message}\n",
        "\n",
        "    # Menyiapkan pesan pengguna untuk ulasan yang ingin diuji\n",
        "    user_message = [{'role': 'user', 'content': f\"{delimiter}{ulasan}{delimiter}\"}]\n",
        "\n",
        "    # Gabungkan pesan sistem dan pesan pengguna\n",
        "    messages = [system_message] + user_message\n",
        "\n",
        "    # Dapatkan respons model\n",
        "    response = get_completion_from_message(messages).strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Tambahkan kolom 'sentimen' berdasarkan hasil dari fungsi sentiment_single\n",
        "df_testing['one-shot_pos'] = df_testing['text_1'].apply(sentiment_single)\n",
        "\n",
        "# Cetak dataframe dengan kolom sentimen yang baru\n",
        "print(df_testing)\n"
      ],
      "metadata": {
        "id": "WQ8CExReCEQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-Shot Negative"
      ],
      "metadata": {
        "id": "vY5_ouwUC9LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_completion_from_message(message,\n",
        "                                model=\"gpt-4o\",\n",
        "                                temperature=0):\n",
        "    client = OpenAI(\n",
        "        api_key='OPENAI_API_KEY',\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=message,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def sentiment_single(ulasan):\n",
        "    delimiter = \"#####\"\n",
        "\n",
        "    # Menyediakan contoh one-shot negatif\n",
        "    example_input = \"5 bintang kalo easy withdraw\"\n",
        "    example_output = \"negatif\"\n",
        "\n",
        "    system_message = f\"\"\"Saya sedang melakukan analisis sentimen terhadap ulasan pelanggan dari sebuah aplikasi \\\n",
        "    exchange crypto bernama tokocrypto.\n",
        "    Berikut adalah contoh ulasan dan jawabannya:\n",
        "    Ulasan: {example_input}\n",
        "    Jawaban: {example_output}\n",
        "\n",
        "    Petunjuk:\n",
        "    - Jawab hanya dengan satu kata: \"positif\" atau \"negatif\" berdasarkan ulasan yang diberikan.\n",
        "    - Ulasan: {delimiter}{ulasan}{delimiter}\"\"\"\n",
        "\n",
        "    system_message = {'role': 'system', 'content': system_message}\n",
        "\n",
        "    # Menyiapkan pesan pengguna untuk ulasan yang ingin diuji\n",
        "    user_message = [{'role': 'user', 'content': f\"{delimiter}{ulasan}{delimiter}\"}]\n",
        "\n",
        "    # Gabungkan pesan sistem dan pesan pengguna\n",
        "    messages = [system_message] + user_message\n",
        "\n",
        "    # Dapatkan respons model\n",
        "    response = get_completion_from_message(messages).strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Tambahkan kolom 'sentimen' berdasarkan hasil dari fungsi sentiment_single\n",
        "df_testing['one-shot_neg'] = df_testing['text_1'].apply(sentiment_single)\n",
        "\n",
        "# Cetak dataframe dengan kolom sentimen yang baru\n",
        "print(df_testing)\n"
      ],
      "metadata": {
        "id": "FSoblNsbC_SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-Shot"
      ],
      "metadata": {
        "id": "eRXDQAegDnIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_completion_from_message(message,\n",
        "                                model=\"gpt-4o\",\n",
        "                                temperature=0):\n",
        "    client = OpenAI(\n",
        "        api_key='OPENAI_API_KEY',\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=message,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def sentiment_single(ulasan):\n",
        "    delimiter = \"#####\"\n",
        "\n",
        "    # Menyediakan beberapa contoh (few-shot)\n",
        "    examples = [\n",
        "        {\"ulasan\": \"ui aplikasinya simple, jadi bikin gabingung buat orang yang baru belajar\", \"sentimen\": \"positif\"},\n",
        "        {\"ulasan\": \"invest mudah kali disini asli wkwkkw\", \"sentimen\": \"positif\"},\n",
        "        {\"ulasan\": \"woah aplikasi amanah simple gampang dipahami pemula belajar recommended\", \"sentimen\": \"positif\"},\n",
        "        {\"ulasan\": \"kalo konversi saldo kecil ke tko udah bisa lagi, saya kasih bintang tujuh\", \"sentimen\": \"negatif\"},\n",
        "        {\"ulasan\": \"butuh widget cepat\", \"sentimen\": \"negatif\"},\n",
        "        {\"ulasan\": \"tolong tambahkan fitur persentase keuntungan / kerugian dari rata rata harga coin saat di beli \\\n",
        "        contoh seperti pluang...\", \"sentimen\":\"negatif\"},\n",
        "        {\"ulasan\": \"keseluruhan sudah sangat baik prosesnya cepat, tidak ada kendala saat pembelian note untuk konversi \\\n",
        "        koin kecil ke tko tolong dibuka secepatnya\", \"sentimen\":\"positif\"},\n",
        "        {\"ulasan\": \"kebalilan fungsi ss!!! fungsi ss penting.\", \"sentimen\":\"negatif\"},\n",
        "        {\"ulasan\": \"lebih baik daripada yg sebelah soal stability tapi soal grafik analysis masih kalah lohh\", \"sentimen\":\"positif\"},\n",
        "        {\"ulasan\": \"versi ipad enggak responsive oii\", \"sentimen\":\"negatif\"},\n",
        "        {\"ulasan\": \"tokocryto sangat bagus untuk bermain jual beli coin\", \"sentimen\":\"positif\"},\n",
        "        {\"ulasan\": \"sudah 2 minggu akun belum diverifikasi juga payah\", \"sentimen\":\"negatif\"},\n",
        "        {\"ulasan\": \"tokocrypto mudah digunakan karena penggunaan ui nya sederhana\", \"sentimen\":\"positif\"},\n",
        "        {\"ulasan\": \"perjuangan banget bisa approved kyc level 1 nya.. udah lama install tpi baru bisa sepenuhnya\\\n",
        "         join...alhamdulillah... smoga bisa lebih cuan cuan cuan lagi disini.. pertanyaan, brapa lama waktu yg \\\n",
        "         dibutuhkan setelah top up terkirim sampai bisa transaksi ya? klo agak lama mungkim bisa dipercepat kah?\", \"sentimen\": \"negatif\"}\n",
        "    ]\n",
        "\n",
        "    # Membuat pesan sistem dengan beberapa contoh\n",
        "    example_messages = \"\"\n",
        "    for example in examples:\n",
        "        example_messages += f\"Ulasan: {example['ulasan']}\\nJawaban: {example['sentimen']}\\n\\n\"\n",
        "\n",
        "    system_message = f\"\"\"Saya sedang melakukan analisis sentimen terhadap ulasan pelanggan dari sebuah aplikasi \\\n",
        "    exchange crypto bernama tokocrypto.\n",
        "    Berikut adalah beberapa contoh ulasan dan jawabannya:\n",
        "    {example_messages}\n",
        "\n",
        "    Petunjuk:\n",
        "    - Jawab hanya dengan satu kata: \"positif\" atau \"negatif\" berdasarkan ulasan yang diberikan.\n",
        "    - Ulasan: {delimiter}{ulasan}{delimiter}\"\"\"\n",
        "\n",
        "    system_message = {'role': 'system', 'content': system_message}\n",
        "\n",
        "    # Menyiapkan pesan pengguna untuk ulasan yang ingin diuji\n",
        "    user_message = [{'role': 'user', 'content': f\"{delimiter}{ulasan}{delimiter}\"}]\n",
        "\n",
        "    # Gabungkan pesan sistem dan pesan pengguna\n",
        "    messages = [system_message] + user_message\n",
        "\n",
        "    # Dapatkan respons model\n",
        "    response = get_completion_from_message(messages).strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Tambahkan kolom 'sentimen' berdasarkan hasil dari fungsi sentiment_single\n",
        "df_testing['few-shot'] = df_testing['text_1'].apply(sentiment_single)\n",
        "\n",
        "# Cetak dataframe dengan kolom sentimen yang baru\n",
        "print(df_testing)\n"
      ],
      "metadata": {
        "id": "uUmyiIZNDoj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis All Data"
      ],
      "metadata": {
        "id": "IlFyIf9EGFbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_from_message(message,\n",
        "                                model=\"gpt-4o\",\n",
        "                                temperature=0):\n",
        "    client = OpenAI(\n",
        "        api_key='OPENAI_API_KEY',\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=message,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def sentiment(ulasan):\n",
        "    delimiter = \"#####\"\n",
        "    system_message = f\"\"\"Saya sedang melakukan analisis sentimen terhadap ulasan pelanggan dari sebuah aplikasi\\\n",
        "     exchange crypto bernama tokocrypto\n",
        "    Petunjuk:\n",
        "    Jawab hanya dengan satu kata saja: \"positif\" atau \"negatif\"!\"\"\"\n",
        "\n",
        "    system_message = {'role': 'system', 'content': system_message}\n",
        "\n",
        "    # Menyiapkan pesan pengguna untuk ulasan yang ingin diuji\n",
        "    user_message = [{'role': 'user', 'content': f\"{delimiter}{ulasan}{delimiter}\"}]\n",
        "\n",
        "    # Gabungkan pesan sistem dan pesan pengguna\n",
        "    messages = [system_message] + user_message\n",
        "\n",
        "    # Dapatkan respons model\n",
        "    response = get_completion_from_message(messages).strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Tambahkan kolom 'sentimen' berdasarkan hasil dari fungsi sentiment\n",
        "df['zero-shot'] = df['text_1'].apply(sentiment)\n",
        "\n",
        "# Cetak dataframe dengan kolom sentimen yang baru\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "LgLszFD2GSMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "LCdggHBkNwU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat tokenizer pretrained dari model IndoBERT-base-p2 untuk tokenisasi teks berbahasa Indonesia\n",
        "tokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
        "inputs_tokocrypto = tokenizer(df[\"text_1\"].tolist(), return_tensors='tf', truncation=True, padding=True, max_length=128)"
      ],
      "metadata": {
        "id": "zTVJeNJzNr7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text_bert(input_tokens, data_name, batch_size=128, load=False):\n",
        "    model = TFAutoModel.from_pretrained('indobenchmark/indobert-base-p2')\n",
        "\n",
        "    if not load:\n",
        "        all_outputs = []\n",
        "        for i in tqdm(range(0, len(input_tokens['input_ids']), batch_size)):\n",
        "            batch = {k: v[i:i+batch_size] for k, v in input_tokens.items()}\n",
        "            outputs = model(batch, output_hidden_states=True)\n",
        "\n",
        "            outputs = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
        "            all_outputs.append(outputs)\n",
        "\n",
        "        final_output = tf.concat(all_outputs, axis=0)\n",
        "        SAVE_DIR = f'{data_name}_BERT.pkl'\n",
        "        joblib.dump(final_output, SAVE_DIR)\n",
        "        return final_output\n",
        "\n",
        "    else:\n",
        "        SAVE_DIR = f'{data_name}_BERT.pkl'\n",
        "        emb = joblib.load(SAVE_DIR)\n",
        "        return emb"
      ],
      "metadata": {
        "id": "Xn2tG7WzOJFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = encode_text_bert(inputs_tokocrypto, 'tokocrypto')"
      ],
      "metadata": {
        "id": "sIV4EPCIOKeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c-TFIDF"
      ],
      "metadata": {
        "id": "nYSNX_6g_0mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Based TFIDF\n",
        "def tf_idf_count(documents, m, ngram_range=(1, 1)):\n",
        "    count = CountVectorizer(ngram_range=ngram_range).fit(documents)\n",
        "    t = count.transform(documents).toarray()\n",
        "    w = t.sum(axis=1)\n",
        "    tf = np.divide(t.T, w)\n",
        "    sum_t = t.sum(axis=0)\n",
        "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
        "    tf_idf = np.multiply(tf, idf)\n",
        "\n",
        "    return tf_idf, count\n",
        "\n",
        "def extract_top_words_per_topic(tf_idf, count, docs_per_topic, n = 10):\n",
        "    words = count.get_feature_names_out()\n",
        "    labels = list(docs_per_topic.topic)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    tf_idf_top_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    top_words = []\n",
        "    for i in range(len(labels)):\n",
        "      top_words.append([words[j] for j in indices [i]][::-1])\n",
        "    return top_words, tf_idf_top_words\n",
        "\n",
        "def extract_topic_sizes(df):\n",
        "    topic_sizes = (df.groupby(['topic'])\n",
        "                     .text\n",
        "                     .count()\n",
        "                     .reset_index()\n",
        "                     .rename({\"topic\": \"topic\", \"text\": \"size\"}, axis='columns')\n",
        "                     .sort_values(\"size\", ascending=False))\n",
        "    return topic_sizes\n",
        "\n",
        "def c_tf_idf(docs_per_topic, m, n_words=10, n_gram_range=(1,1)):\n",
        "    tf_idf, count = tf_idf_count(docs_per_topic.text.values, m=m)\n",
        "    top_words, tf_idf_top_words = extract_top_words_per_topic(tf_idf, count, docs_per_topic, n_words)\n",
        "\n",
        "    return tf_idf, count, top_words, tf_idf_top_words"
      ],
      "metadata": {
        "id": "21UeWIE2Prr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT EFCM c-TFIDF"
      ],
      "metadata": {
        "id": "pRdx9avaAhEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BERT_EFCM_cTFIDF(df, data_name, n_clusters, m=1.1, n_words=10,\n",
        "                     n_gram_range=(1,1)):\n",
        "\n",
        "    # BERT\n",
        "    print('Creating Text Embedding using BERT...')\n",
        "    emb = encode_text_bert(df, data_name='tokocrypto', load=True)\n",
        "\n",
        "    # EFCM\n",
        "    print('Training EFCM...')\n",
        "\n",
        "    svd = TruncatedSVD(n_components = 5)\n",
        "    emb_tsvd = svd.fit_transform(emb)\n",
        "\n",
        "    initkm = KMeans(n_clusters=n_clusters, n_init=1).fit(emb_tsvd)\n",
        "    cntr, u = fcmeans(emb_tsvd.T, n_clusters, m, error=0.0001, maxiter=200, init=initkm.cluster_centers_.T)\n",
        "    cluster_membership = np.argmax(u, axis=0)\n",
        "\n",
        "    # cTFIDF\n",
        "    print('Extracting top words using cTFIDF')\n",
        "    docs_df = df[['text_2']].copy()\n",
        "    docs_df.rename({'text_2': 'text'}, axis=1, inplace=True)\n",
        "\n",
        "    docs_df['topic'] = cluster_membership.reshape(-1,1)\n",
        "\n",
        "    docs_df['doc_id'] = range(len(docs_df))\n",
        "\n",
        "    docs_per_topic = docs_df.dropna(subset=['text']).groupby(['topic'], as_index = False).agg({'text': ' '.join})\n",
        "\n",
        "    c_tf_idf_, count, top_words, top_words_c_tf_idf = c_tf_idf(docs_per_topic, docs_df.shape[0],\n",
        "                                                              n_words=n_words, n_gram_range=n_gram_range)\n",
        "\n",
        "    print('FINISHED')\n",
        "\n",
        "    return emb, emb_tsvd, cluster_membership, c_tf_idf_, count, top_words, top_words_c_tf_idf"
      ],
      "metadata": {
        "id": "_bxw8Gn1PvsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TC-W2V"
      ],
      "metadata": {
        "id": "xWSeqyhrANfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Nilai Coherence\n",
        "### Memuat Model Word2Vec\n",
        "w2v_model = gensim.models.Word2Vec.load(\"/content/drive/MyDrive/Deep Learning/Data/word2vec/idwiki-berita/w2v-model.bin\")\n",
        "\n",
        "def calculate_coherence(w2v_model, term_rankings):\n",
        "    overall_coherence = 0.0\n",
        "    for topic_index in range(len(term_rankings)):\n",
        "        # check each pair of terms\n",
        "        pair_scores = []\n",
        "        for i in range(len(term_rankings[topic_index])):\n",
        "            for j in range(i + 1, len(term_rankings[topic_index])):\n",
        "                term_i = term_rankings[topic_index][i]\n",
        "                term_j = term_rankings[topic_index][j]\n",
        "                if term_i in w2v_model.wv.key_to_index and term_j in w2v_model.wv.key_to_index:\n",
        "                    pair_scores.append(w2v_model.wv.similarity(term_i, term_j))\n",
        "\n",
        "        # get the mean for all pairs in this topic\n",
        "        if pair_scores:\n",
        "            topic_score = sum(pair_scores) / len(pair_scores)\n",
        "            overall_coherence += topic_score\n",
        "\n",
        "    # get the mean score across all topics\n",
        "    return overall_coherence / len(term_rankings)"
      ],
      "metadata": {
        "id": "PUOsbucHP3ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EFCM"
      ],
      "metadata": {
        "id": "2rU7FeppAyA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat looping untuk parameter m dari 1.05 sampai 3.00 dengan interval 0.05\n",
        "for m in np.arange(1.05, 3.00, 0.05):\n",
        "    print('=' * 100)\n",
        "    print('Fuzziness:', round(m, 2))\n",
        "\n",
        "    tc_w2v = []\n",
        "    best_tc_w2v = 0\n",
        "    all_tc_w2v_means = []\n",
        "\n",
        "    for n_topic in range(1, 11):\n",
        "        print('=' * 100)\n",
        "        print('TOPIC:', n_topic)\n",
        "\n",
        "        tc_w2v_sim = []\n",
        "        best_tc_w2v_sim = 0\n",
        "\n",
        "        for sim in range(2):\n",
        "            print('=' * 100)\n",
        "            print('SIMULATION:', sim + 1)\n",
        "            print('=' * 100)\n",
        "\n",
        "            # Memanggil fungsi BERT_EFCM_cTFIDF function\n",
        "            emb_, emb_tsvd_, cluster_member_, c_tf_idf_, count_, top_words_, top_words_c_tf_idf_ = BERT_EFCM_cTFIDF(\n",
        "                df,\n",
        "                m=round(m, 2),\n",
        "                data_name='tokocrypto',\n",
        "                n_clusters=n_topic\n",
        "            )\n",
        "\n",
        "            # Menghitung nilai coherence\n",
        "            coherence = calculate_coherence(w2v_model, top_words_)\n",
        "            tc_w2v_sim.append(coherence)\n",
        "\n",
        "            if best_tc_w2v_sim < coherence:\n",
        "                best_tc_w2v_sim = coherence\n",
        "                best_emb_tsvd_sim, best_cluster_member_sim, best_c_tf_idf_sim, best_count_sim, best_top_words_sim, \\\n",
        "                best_top_words_ctfidf_sim = (\n",
        "                    emb_tsvd_, cluster_member_, c_tf_idf_, count_, top_words_, top_words_c_tf_idf_\n",
        "                )\n",
        "\n",
        "            print('=' * 100)\n",
        "            print(f'FINISHED; {n_topic} TOPICS; SIMULATION {sim + 1}; TC-W2V {round(tc_w2v_sim[-1], 3)}')\n",
        "            print(f'BEST CURRENT TC-W2V: {best_tc_w2v_sim}; TOP WORDS: {best_top_words_sim}')\n",
        "\n",
        "        tc_w2v.append(tc_w2v_sim)\n",
        "\n",
        "        mean_tc_w2v = np.array(tc_w2v_sim).mean()\n",
        "        all_tc_w2v_means.append(mean_tc_w2v)\n",
        "\n",
        "        if best_tc_w2v < mean_tc_w2v:\n",
        "            best_tc_w2v = mean_tc_w2v\n",
        "            best_emb_tsvd, best_cluster_member, best_c_tf_idf, best_count, best_top_words, best_top_words_ctfidf = (\n",
        "                best_emb_tsvd_sim, best_cluster_member_sim, best_c_tf_idf_sim, best_count_sim, best_top_words_sim,\\\n",
        "                best_top_words_ctfidf_sim\n",
        "            )\n",
        "            best_n_topic = n_topic\n",
        "\n",
        "        print('-' * 100)\n",
        "        print(f'FINISHED; {n_topic} TOPICS ALL SIMULATION; TC-W2V mean {mean_tc_w2v}')\n",
        "        print(f'BEST N TOPICS: {best_n_topic}; BEST TC-W2V: {best_tc_w2v}; BEST TOP WORDS: {best_top_words}')\n",
        "        print('-' * 100)\n",
        "\n",
        "    # Membuat grafik\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, 11), all_tc_w2v_means, marker='o', label=f'm={round(m, 2)}')\n",
        "    plt.title(f'Topic Coherence vs Number of Topics (m={round(m, 2)})')\n",
        "    plt.xlabel('Number of Topics')\n",
        "    plt.ylabel('Mean TC-W2V')\n",
        "    plt.xticks(range(1, 11))\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ED2sOdM6QBTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan parameter m yang dipilih\n",
        "best_m = 2.35\n",
        "best_n_topic = best_n_topic\n",
        "\n",
        "print('=' * 100)\n",
        "print(f'Applying BERT_EFCM_cTFIDF with Best Parameters: m={round(best_m, 2)}, n_clusters={best_n_topic}')\n",
        "\n",
        "emb, emb_tsvd, cluster_membership, c_tf_idf_, count, top_words, top_words_c_tf_idf = BERT_EFCM_cTFIDF(\n",
        "    df,\n",
        "    data_name='tokocrypto',\n",
        "    n_clusters=best_n_topic,\n",
        "    m=round(best_m, 2)\n",
        ")\n",
        "\n",
        "print('BERT_EFCM_cTFIDF applied successfully.')"
      ],
      "metadata": {
        "id": "d5NEkMRRQTBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan cluster pada dataset\n",
        "results_df = df.copy()\n",
        "results_df['topics'] = cluster_membership\n",
        "\n",
        "results_df[['text_1','text_2', 'topics']]"
      ],
      "metadata": {
        "id": "IMO9oZ91QXa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah ulasan untuk setiap topik (cluster)\n",
        "topic_counts = results_df['topics'].value_counts()\n",
        "print(topic_counts)"
      ],
      "metadata": {
        "id": "ZdTlmb-QQaxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top Words"
      ],
      "metadata": {
        "id": "K9ZWJMayAU-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat dictionary untuk menyimpan kata-kata teratas dari setiap topik\n",
        "topics_words = {}\n",
        "\n",
        "# Looping untuk setiap topik (misalnya, ada 3 topik dalam top_words_c_tf_idf)\n",
        "for i in range(len(top_words_c_tf_idf)):\n",
        "    # Menyimpan kata-kata teratas dari setiap topik ke dalam dictionary\n",
        "    topics_words[f\"Topic {i}\"] = ', '.join(word[0] for word in top_words_c_tf_idf[i])\n",
        "\n",
        "# Menampilkan hasil\n",
        "for topic, words in topics_words.items():\n",
        "    print(f\"{topic}: {words}\")"
      ],
      "metadata": {
        "id": "wUbFT-7nQcjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dokumen Representatif"
      ],
      "metadata": {
        "id": "Vvvb5AyrUJVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_vectors(ctfidf_matrix, topic_words, word_index):\n",
        "    # Jumlah topik\n",
        "    num_topics = ctfidf_matrix.shape[1]\n",
        "\n",
        "    vocab_size = ctfidf_matrix.shape[0]\n",
        "\n",
        "    topic_vectors = np.zeros((num_topics, vocab_size))\n",
        "\n",
        "    for topic_idx, words in enumerate(topic_words):\n",
        "        for word in words:\n",
        "            if word in word_index:\n",
        "                word_idx = word_index[word]\n",
        "                topic_vectors[topic_idx, word_idx] = ctfidf_matrix[word_idx, topic_idx]\n",
        "\n",
        "    return topic_vectors\n",
        "\n",
        "topic_vectors = get_topic_vectors(best_c_tf_idf, best_top_words, best_count.vocabulary_)"
      ],
      "metadata": {
        "id": "l790GWMAQfDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    dot_product = np.dot(b, a.T)\n",
        "\n",
        "    norm_b = np.linalg.norm(b, axis=1)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "\n",
        "    if norm_a == 0 or np.any(norm_b == 0):\n",
        "        return np.array([0] * b.shape[0])\n",
        "\n",
        "    cosine_similarity = dot_product.ravel() / (norm_b * norm_a)\n",
        "\n",
        "    return cosine_similarity"
      ],
      "metadata": {
        "id": "YJwvHLkfQf_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df['text_2'] = results_df['text_2'].astype(str)\n",
        "sim_scores = []\n",
        "for i in tqdm(range(results_df.shape[0])):\n",
        "    docs_vector = np.zeros(best_c_tf_idf.shape[0])\n",
        "    for word in results_df['text_2'].values[i].split():\n",
        "            if word in best_count.vocabulary_:\n",
        "                word_idx = best_count.vocabulary_[word]\n",
        "                topic_idx = results_df['topics'].values[i]\n",
        "                docs_vector[word_idx] = best_c_tf_idf[word_idx, topic_idx]\n",
        "\n",
        "    sim_scores.append(cosine_similarity(topic_vectors[topic_idx].reshape(1,-1), docs_vector.reshape(1,-1))[0])\n",
        "\n",
        "results_df['scores'] = sim_scores\n",
        "results_df"
      ],
      "metadata": {
        "id": "p6Z0zaAtQjJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_top_documents_by_topic(df, num_top_docs=5):\n",
        "    unique_topics = sorted(df['topics'].unique())\n",
        "\n",
        "    for topic in unique_topics:\n",
        "        print(f\"Topics: {topic}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        top_docs = df[df['topics'] == topic].nlargest(num_top_docs, 'scores')\n",
        "\n",
        "        for index, row in top_docs.iterrows():\n",
        "            print(f\"{index + 1}. {row['text_1']} - Score: {row['scores']:.6f}\")\n",
        "\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "print_top_documents_by_topic(results_df, num_top_docs=6)"
      ],
      "metadata": {
        "id": "dyy0BZkKTkAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretasi Topik dengan GPT"
      ],
      "metadata": {
        "id": "bYaVRKKQTqEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi OpenAI API\n",
        "openai.api_key = \"OPENAI_API_KEY\"\n",
        "\n",
        "# Jumlah topik\n",
        "num_topics = len(top_words_c_tf_idf)\n",
        "\n",
        "# Loop untuk setiap topik dan membuat prompt untuk GPT\n",
        "for topic_num in range(num_topics):\n",
        "    # Mendapatkan kata-kata teratas untuk topik saat ini\n",
        "    topic_words = topics_words[f\"Topic {topic_num}\"]\n",
        "\n",
        "    # Mendapatkan dokumen teratas untuk topik saat ini\n",
        "    representative_docs = \"\\n\".join([f\"{i+1}. {doc}\" \\\n",
        "                                     for i, doc in enumerate(top_docs_per_topic[f\"Topic {topic_num}\"])])\n",
        "\n",
        "    # Membuat prompt\n",
        "    message = f\"\"\"Tentukan interpretasi topik dari {topic_words} dengan mengacu pada dokumen {representative_docs},\\\n",
        "     Tuliskan interpretasi dalam satu kalimat singkat (maksimal 15 kata), dan menggunakan bahasa Indonesia.\"\"\"\n",
        "\n",
        "    # Membuat permintaan ke OpenAI GPT\n",
        "    completion = openai.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        seed=42,\n",
        "        messages=[{\"role\": \"user\", \"content\": message}]\n",
        "    )\n",
        "\n",
        "    # Menampilkan hasil interpretasi dari GPT\n",
        "    label = completion.choices[0].message.content\n",
        "    print(f\"Interpretasi untuk Topic {topic_num}: {label}\")\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "id": "Y4ai1DFCTtfy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}